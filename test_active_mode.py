#!/usr/bin/env python3
"""
Tests dÃ©diÃ©s au Mode Conversationnel Actif

Ce script teste spÃ©cifiquement les fonctionnalitÃ©s du mode actif
oÃ¹ l'agent poste rÃ©ellement sur Reddit (ou simule en dry run).
"""

import sys
import os
import time
from pathlib import Path

# Ajouter le dossier src au path
current_dir = Path(__file__).parent
src_dir = current_dir / "src"
sys.path.insert(0, str(src_dir))

# Changer le rÃ©pertoire de travail vers le dossier du projet
os.chdir(current_dir)

from config_manager import ConfigManager, setup_logging
from reddit_ai_agent import RedditAIAgent
from active_data_manager import ActiveDataManager


def test_active_data_manager():
    """Test du gestionnaire de donnÃ©es actif"""
    print("=== Test du Gestionnaire de DonnÃ©es Actif ===")
    
    try:
        # CrÃ©er le gestionnaire
        data_manager = ActiveDataManager()
        print("âœ… Gestionnaire de donnÃ©es crÃ©Ã©")
        
        # Test de sauvegarde d'un post scrapÃ©
        test_post = {
            'id': 'test_post_123',
            'title': 'Test Post Title',
            'selftext': 'Test post content',
            'author': 'test_author',
            'score': 10,
            'num_comments': 5,
            'created_utc': time.time(),
            'url': 'https://reddit.com/test',
            'is_self': True
        }
        
        test_analysis = {
            'score': 0.8,
            'should_respond': True,
            'confidence': 0.75,
            'reasons': ['test_trigger']
        }
        
        success = data_manager.log_scraped_post(test_post, 'test', test_analysis)
        if success:
            print("âœ… Post scrapÃ© sauvegardÃ©")
        else:
            print("âŒ Ã‰chec sauvegarde post scrapÃ©")
            return False
        
        # Test de sauvegarde d'une rÃ©ponse postÃ©e
        success = data_manager.log_posted_response(
            'test_post_123', 'test_response_456', 'test',
            'Test response content', 0.85, ['test_trigger']
        )
        if success:
            print("âœ… RÃ©ponse postÃ©e sauvegardÃ©e")
        else:
            print("âŒ Ã‰chec sauvegarde rÃ©ponse postÃ©e")
            return False
        
        # Test de sauvegarde d'un post crÃ©Ã©
        success = data_manager.log_created_post(
            'test_created_789', 'test', 'Test Created Post',
            'Test created content', 'discussion'
        )
        if success:
            print("âœ… Post crÃ©Ã© sauvegardÃ©")
        else:
            print("âŒ Ã‰chec sauvegarde post crÃ©Ã©")
            return False
        
        # Test de log d'action
        success = data_manager.log_action(
            'test_action', 'test', 'test_target', 'test_result',
            {'test_key': 'test_value'}, True, None, 1.5
        )
        if success:
            print("âœ… Action loggÃ©e")
        else:
            print("âŒ Ã‰chec log action")
            return False
        
        # Test des statistiques
        stats = data_manager.get_database_stats()
        print(f"ğŸ“Š Stats DB: {stats}")
        
        # Test de finalisation
        data_manager.finalize_session()
        print("âœ… Session finalisÃ©e")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur dans le test du gestionnaire: {e}")
        return False


def test_agent_initialization():
    """Test de l'initialisation de l'agent avec le gestionnaire de donnÃ©es"""
    print("\n=== Test d'Initialisation de l'Agent ===")
    
    try:
        # Charger la configuration
        config_manager = ConfigManager('config/config.yaml')
        if not config_manager.load_config():
            print("âŒ Ã‰chec chargement configuration")
            return False
        
        print("âœ… Configuration chargÃ©e")
        
        # CrÃ©er l'agent
        agent = RedditAIAgent('config/config.yaml')
        print("âœ… Agent crÃ©Ã©")
        
        # Initialiser (sans dÃ©marrer)
        if not agent.initialize():
            print("âŒ Ã‰chec initialisation agent")
            return False
        
        print("âœ… Agent initialisÃ©")
        
        # VÃ©rifier que le gestionnaire de donnÃ©es est prÃ©sent
        if agent.data_manager is None:
            print("âŒ Gestionnaire de donnÃ©es manquant")
            return False
        
        print("âœ… Gestionnaire de donnÃ©es prÃ©sent")
        
        # Test des statistiques de l'agent
        status = agent.get_status()
        print(f"ğŸ“Š Statut agent: {status}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur dans le test d'initialisation: {e}")
        return False


def test_dry_run_mode():
    """Test du mode dry run avec sauvegarde"""
    print("\n=== Test du Mode Dry Run avec Sauvegarde ===")
    
    try:
        # VÃ©rifier que le mode dry run est activÃ©
        config_manager = ConfigManager('config/config.yaml')
        if not config_manager.load_config():
            print("âŒ Ã‰chec chargement configuration")
            return False
        
        if not config_manager.is_dry_run():
            print("âš ï¸  Mode dry run non activÃ© - test sautÃ© pour sÃ©curitÃ©")
            return True
        
        print("âœ… Mode dry run confirmÃ©")
        
        # CrÃ©er l'agent
        agent = RedditAIAgent('config/config.yaml')
        
        if not agent.initialize():
            print("âŒ Ã‰chec initialisation agent")
            return False
        
        print("âœ… Agent initialisÃ© en mode dry run")
        
        # Simuler un cycle court de surveillance
        enabled_subreddits = config_manager.get_enabled_subreddits()
        if not enabled_subreddits:
            print("âš ï¸  Aucun subreddit activÃ©")
            return True
        
        # Prendre le premier subreddit activÃ©
        test_subreddit = enabled_subreddits[0]
        print(f"ğŸ¯ Test sur r/{test_subreddit.name}")
        
        # Surveiller pendant 30 secondes
        print("â±ï¸  Surveillance de 30 secondes...")
        agent.running = True
        start_time = time.time()
        
        while time.time() - start_time < 30 and agent.running:
            try:
                agent._monitor_subreddit(test_subreddit)
                time.sleep(10)  # Attendre 10 secondes entre les cycles
            except Exception as e:
                print(f"âš ï¸  Erreur pendant la surveillance: {e}")
                break
        
        # ArrÃªter l'agent
        agent.running = False
        agent._shutdown()
        
        # VÃ©rifier les donnÃ©es sauvegardÃ©es
        if agent.data_manager:
            stats = agent.data_manager.get_session_stats()
            print(f"ğŸ“Š RÃ©sultats du test:")
            print(f"   Posts scrapÃ©s: {stats['posts_scraped']}")
            print(f"   Actions simulÃ©es: {stats['actions_performed']}")
            print(f"   Erreurs: {stats['errors_encountered']}")
            
            if stats['posts_scraped'] > 0:
                print("âœ… Posts scrapÃ©s et sauvegardÃ©s")
            else:
                print("âš ï¸  Aucun post scrapÃ© - normal si subreddit peu actif")
            
            if stats['actions_performed'] >= 0:
                print("âœ… Actions trackÃ©es")
            
            return True
        else:
            print("âŒ Gestionnaire de donnÃ©es manquant")
            return False
        
    except Exception as e:
        print(f"âŒ Erreur dans le test dry run: {e}")
        return False


def test_data_persistence():
    """Test de la persistance des donnÃ©es"""
    print("\n=== Test de Persistance des DonnÃ©es ===")
    
    try:
        # CrÃ©er un premier gestionnaire et ajouter des donnÃ©es
        data_manager1 = ActiveDataManager()
        
        # Ajouter des donnÃ©es de test
        test_post = {
            'id': 'persistence_test_123',
            'title': 'Persistence Test Post',
            'selftext': 'Test content',
            'author': 'test_user',
            'score': 15,
            'num_comments': 3,
            'created_utc': time.time(),
            'url': 'https://reddit.com/persistence_test',
            'is_self': True
        }
        
        test_analysis = {
            'score': 0.9,
            'should_respond': True,
            'confidence': 0.8,
            'reasons': ['persistence_test']
        }
        
        success = data_manager1.log_scraped_post(test_post, 'test', test_analysis)
        if not success:
            print("âŒ Ã‰chec ajout donnÃ©es test")
            return False
        
        # RÃ©cupÃ©rer les stats
        stats1 = data_manager1.get_database_stats()
        posts_count1 = stats1.get('total_posts_scraped', 0)
        print(f"ğŸ“Š DonnÃ©es ajoutÃ©es: {posts_count1} posts")
        
        # CrÃ©er un second gestionnaire (nouvelle instance)
        data_manager2 = ActiveDataManager()
        
        # VÃ©rifier que les donnÃ©es persistent
        stats2 = data_manager2.get_database_stats()
        posts_count2 = stats2.get('total_posts_scraped', 0)
        
        if posts_count2 >= posts_count1:
            print("âœ… DonnÃ©es persistÃ©es entre instances")
            return True
        else:
            print(f"âŒ Perte de donnÃ©es: {posts_count1} -> {posts_count2}")
            return False
        
    except Exception as e:
        print(f"âŒ Erreur dans le test de persistance: {e}")
        return False


def test_performance_tracking():
    """Test du suivi des performances Reddit"""
    print("\n=== Test du Suivi des Performances ===")
    
    try:
        data_manager = ActiveDataManager()
        
        # Test de mise Ã  jour des performances
        test_content_id = 'perf_test_123'
        
        # Performance initiale
        success = data_manager.update_reddit_performance(
            test_content_id, 'response', 'test', 5, 2
        )
        if not success:
            print("âŒ Ã‰chec mise Ã  jour performance initiale")
            return False
        
        print("âœ… Performance initiale enregistrÃ©e")
        
        # Mise Ã  jour avec amÃ©lioration
        success = data_manager.update_reddit_performance(
            test_content_id, 'response', 'test', 10, 4
        )
        if not success:
            print("âŒ Ã‰chec mise Ã  jour performance")
            return False
        
        print("âœ… Performance mise Ã  jour (tendance positive)")
        
        # Mise Ã  jour avec baisse
        success = data_manager.update_reddit_performance(
            test_content_id, 'response', 'test', 8, 4
        )
        if not success:
            print("âŒ Ã‰chec mise Ã  jour performance")
            return False
        
        print("âœ… Performance mise Ã  jour (tendance nÃ©gative)")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur dans le test de performance: {e}")
        return False


def test_configuration_validation():
    """Test de validation de la configuration pour le mode actif"""
    print("\n=== Test de Validation Configuration ===")
    
    try:
        config_manager = ConfigManager('config/config.yaml')
        if not config_manager.load_config():
            print("âŒ Ã‰chec chargement configuration")
            return False
        
        print("âœ… Configuration chargÃ©e")
        
        # VÃ©rifier les Ã©lÃ©ments critiques
        if not config_manager.reddit_config:
            print("âŒ Configuration Reddit manquante")
            return False
        
        print("âœ… Configuration Reddit prÃ©sente")
        
        if not config_manager.lm_studio_config:
            print("âŒ Configuration LM Studio manquante")
            return False
        
        print("âœ… Configuration LM Studio prÃ©sente")
        
        # VÃ©rifier les subreddits
        enabled_subreddits = config_manager.get_enabled_subreddits()
        if not enabled_subreddits:
            print("âš ï¸  Aucun subreddit activÃ©")
        else:
            print(f"âœ… {len(enabled_subreddits)} subreddits activÃ©s")
        
        # VÃ©rifier les triggers
        triggers = config_manager.get_triggers()
        if not triggers:
            print("âš ï¸  Aucun trigger configurÃ©")
        else:
            keywords = triggers.get('keywords', [])
            print(f"âœ… {len(keywords)} mots-clÃ©s triggers configurÃ©s")
        
        # VÃ©rifier les modes
        print(f"ğŸ“Š Mode dry run: {config_manager.is_dry_run()}")
        print(f"ğŸ“Š Mode debug: {config_manager.is_debug()}")
        print(f"ğŸ“Š Mode monitor only: {config_manager.is_monitor_only()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur dans la validation config: {e}")
        return False


def run_comprehensive_test():
    """Lance tous les tests du mode actif"""
    print("ğŸ§ª TESTS COMPLETS DU MODE ACTIF")
    print("=" * 50)
    
    tests = [
        ("Gestionnaire de DonnÃ©es", test_active_data_manager),
        ("Initialisation Agent", test_agent_initialization),
        ("Mode Dry Run", test_dry_run_mode),
        ("Persistance DonnÃ©es", test_data_persistence),
        ("Suivi Performances", test_performance_tracking),
        ("Validation Configuration", test_configuration_validation)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"\nğŸ” Test: {test_name}")
        print("-" * 40)
        
        try:
            start_time = time.time()
            success = test_func()
            duration = time.time() - start_time
            
            results[test_name] = {
                'success': success,
                'duration': round(duration, 2)
            }
            
            status = "âœ… RÃ‰USSI" if success else "âŒ Ã‰CHOUÃ‰"
            print(f"{status} ({duration:.2f}s)")
            
        except Exception as e:
            results[test_name] = {
                'success': False,
                'duration': 0,
                'error': str(e)
            }
            print(f"âŒ ERREUR: {e}")
    
    # RÃ©sumÃ© final
    print("\n" + "=" * 50)
    print("ğŸ“‹ RÃ‰SUMÃ‰ DES TESTS MODE ACTIF")
    print("=" * 50)
    
    total_tests = len(tests)
    passed_tests = sum(1 for r in results.values() if r['success'])
    
    for test_name, result in results.items():
        status = "âœ…" if result['success'] else "âŒ"
        duration = result['duration']
        print(f"{status} {test_name:<25} ({duration:.2f}s)")
        
        if not result['success'] and 'error' in result:
            print(f"   Erreur: {result['error']}")
    
    print(f"\nğŸ¯ RÃ©sultat: {passed_tests}/{total_tests} tests rÃ©ussis")
    
    if passed_tests == total_tests:
        print("ğŸ‰ TOUS LES TESTS SONT PASSÃ‰S!")
        print("\nLe mode actif est prÃªt Ã  Ãªtre utilisÃ©:")
        print("  python run_active_mode.py --duration 2")
        print("  python run_active_mode.py --stats")
        return True
    else:
        print("âš ï¸  Certains tests ont Ã©chouÃ©. VÃ©rifiez la configuration.")
        return False


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Tests du Mode Actif')
    parser.add_argument('--data-manager', action='store_true',
                       help='Test du gestionnaire de donnÃ©es seulement')
    parser.add_argument('--agent', action='store_true',
                       help='Test de l\'agent seulement')
    parser.add_argument('--dry-run', action='store_true',
                       help='Test du mode dry run seulement')
    parser.add_argument('--persistence', action='store_true',
                       help='Test de persistance seulement')
    parser.add_argument('--performance', action='store_true',
                       help='Test de performance seulement')
    parser.add_argument('--config', action='store_true',
                       help='Test de configuration seulement')
    
    args = parser.parse_args()
    
    if args.data_manager:
        return 0 if test_active_data_manager() else 1
    elif args.agent:
        return 0 if test_agent_initialization() else 1
    elif args.dry_run:
        return 0 if test_dry_run_mode() else 1
    elif args.persistence:
        return 0 if test_data_persistence() else 1
    elif args.performance:
        return 0 if test_performance_tracking() else 1
    elif args.config:
        return 0 if test_configuration_validation() else 1
    else:
        return 0 if run_comprehensive_test() else 1


if __name__ == "__main__":
    sys.exit(main())

